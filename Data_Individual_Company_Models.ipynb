{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker  SimFinId        Date   Open   High    Low  Close  Adj. Close  \\\n",
      "0      A     45846  2019-04-05  81.19  81.92  81.05  81.47       78.25   \n",
      "1      A     45846  2019-04-08  81.57  81.71  80.58  81.69       78.46   \n",
      "2      A     45846  2019-04-09  81.56  81.72  81.27  81.42       78.20   \n",
      "3      A     45846  2019-04-10  81.45  82.24  81.45  81.68       78.45   \n",
      "4      A     45846  2019-04-11  81.88  81.92  80.89  81.08       77.87   \n",
      "\n",
      "    Volume  Dividend  Shares Outstanding  \n",
      "0  1502875       NaN         317515869.0  \n",
      "1   783350       NaN         317515869.0  \n",
      "2  1254742       NaN         317515869.0  \n",
      "3   982886       NaN         317515869.0  \n",
      "4  1071479       NaN         317515869.0  \n"
     ]
    }
   ],
   "source": [
    "# opening the simfin data\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"data/us-shareprices-daily.csv\"\n",
    "\n",
    "# Read the CSV file with semicolon separator\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker  SimFinId        Date   Open   High    Low  Close  Adj. Close  \\\n",
      "0      A     45846  2019-04-05  81.19  81.92  81.05  81.47       78.25   \n",
      "1      A     45846  2019-04-08  81.57  81.71  80.58  81.69       78.46   \n",
      "2      A     45846  2019-04-09  81.56  81.72  81.27  81.42       78.20   \n",
      "3      A     45846  2019-04-10  81.45  82.24  81.45  81.68       78.45   \n",
      "4      A     45846  2019-04-11  81.88  81.92  80.89  81.08       77.87   \n",
      "\n",
      "    Volume  Dividend  Shares Outstanding  \n",
      "0  1502875       NaN         317515869.0  \n",
      "1   783350       NaN         317515869.0  \n",
      "2  1254742       NaN         317515869.0  \n",
      "3   982886       NaN         317515869.0  \n",
      "4  1071479       NaN         317515869.0  \n"
     ]
    }
   ],
   "source": [
    "# opening company data\n",
    "# Define the file path\n",
    "file_path2 = \"data/us-companies.csv\"\n",
    "\n",
    "# Read the CSV file with semicolon separator\n",
    "df2 = pd.read_csv(file_path2, sep=';')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker_x  SimFinId              Company Name  IndustryId          ISIN  \\\n",
      "0        A     45846  AGILENT TECHNOLOGIES INC    106001.0  US00846U1016   \n",
      "1        A     45846  AGILENT TECHNOLOGIES INC    106001.0  US00846U1016   \n",
      "2        A     45846  AGILENT TECHNOLOGIES INC    106001.0  US00846U1016   \n",
      "3        A     45846  AGILENT TECHNOLOGIES INC    106001.0  US00846U1016   \n",
      "4        A     45846  AGILENT TECHNOLOGIES INC    106001.0  US00846U1016   \n",
      "\n",
      "   End of financial year (month)  Number Employees  \\\n",
      "0                           10.0           16400.0   \n",
      "1                           10.0           16400.0   \n",
      "2                           10.0           16400.0   \n",
      "3                           10.0           16400.0   \n",
      "4                           10.0           16400.0   \n",
      "\n",
      "                                    Business Summary Market        CIK  ...  \\\n",
      "0  Agilent Technologies Inc is engaged in life sc...     us  1090872.0  ...   \n",
      "1  Agilent Technologies Inc is engaged in life sc...     us  1090872.0  ...   \n",
      "2  Agilent Technologies Inc is engaged in life sc...     us  1090872.0  ...   \n",
      "3  Agilent Technologies Inc is engaged in life sc...     us  1090872.0  ...   \n",
      "4  Agilent Technologies Inc is engaged in life sc...     us  1090872.0  ...   \n",
      "\n",
      "  Ticker_y        Date   Open   High    Low  Close  Adj. Close   Volume  \\\n",
      "0        A  2019-04-05  81.19  81.92  81.05  81.47       78.25  1502875   \n",
      "1        A  2019-04-08  81.57  81.71  80.58  81.69       78.46   783350   \n",
      "2        A  2019-04-09  81.56  81.72  81.27  81.42       78.20  1254742   \n",
      "3        A  2019-04-10  81.45  82.24  81.45  81.68       78.45   982886   \n",
      "4        A  2019-04-11  81.88  81.92  80.89  81.08       77.87  1071479   \n",
      "\n",
      "   Dividend  Shares Outstanding  \n",
      "0       NaN         317515869.0  \n",
      "1       NaN         317515869.0  \n",
      "2       NaN         317515869.0  \n",
      "3       NaN         317515869.0  \n",
      "4       NaN         317515869.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge dfs\n",
    "merged_df = pd.merge(df2, df, on=\"SimFinId\", how=\"inner\") \n",
    "# Display the first few rows\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Date\"] = pd.to_datetime(merged_df[\"Date\"], format='%Y-%m-%d')\n",
    "\n",
    "# Transform Company Name into a string\n",
    "merged_df[\"Company Name\"] = merged_df[\"Company Name\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker_x              Company Name  IndustryId  Number Employees       Date  \\\n",
      "0        A  AGILENT TECHNOLOGIES INC    106001.0           16400.0 2019-04-05   \n",
      "1        A  AGILENT TECHNOLOGIES INC    106001.0           16400.0 2019-04-08   \n",
      "2        A  AGILENT TECHNOLOGIES INC    106001.0           16400.0 2019-04-09   \n",
      "3        A  AGILENT TECHNOLOGIES INC    106001.0           16400.0 2019-04-10   \n",
      "4        A  AGILENT TECHNOLOGIES INC    106001.0           16400.0 2019-04-11   \n",
      "\n",
      "    Open   High    Low  Close  Adj. Close   Volume  Dividend  \\\n",
      "0  81.19  81.92  81.05  81.47       78.25  1502875       NaN   \n",
      "1  81.57  81.71  80.58  81.69       78.46   783350       NaN   \n",
      "2  81.56  81.72  81.27  81.42       78.20  1254742       NaN   \n",
      "3  81.45  82.24  81.45  81.68       78.45   982886       NaN   \n",
      "4  81.88  81.92  80.89  81.08       77.87  1071479       NaN   \n",
      "\n",
      "   Shares Outstanding  \n",
      "0         317515869.0  \n",
      "1         317515869.0  \n",
      "2         317515869.0  \n",
      "3         317515869.0  \n",
      "4         317515869.0  \n"
     ]
    }
   ],
   "source": [
    "# Define the list of columns to keep\n",
    "columns_to_keep = [\"Ticker_x\",\n",
    "    \"Company Name\", \"IndustryId\", \"Number Employees\", \"Date\", \"Open\", \n",
    "    \"High\", \"Low\", \"Close\", \"Adj. Close\", \"Volume\", \"Dividend\", \"Shares Outstanding\"\n",
    "]\n",
    "\n",
    "# Select only the specified columns\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker_x              0.000000\n",
      "Company Name          0.000000\n",
      "IndustryId            0.912000\n",
      "Number Employees      5.200318\n",
      "Date                  0.000000\n",
      "Open                  0.000000\n",
      "High                  0.000000\n",
      "Low                   0.000000\n",
      "Close                 0.000000\n",
      "Adj. Close            0.000000\n",
      "Volume                0.000000\n",
      "Dividend              0.000000\n",
      "Shares Outstanding    8.956522\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Change all Dividend missing in merged to 0 and then try to fix others\n",
    "merged_df[\"Dividend\"] = merged_df[\"Dividend\"].fillna(0)\n",
    "\n",
    "# Calculate the percentage of missing values per column\n",
    "missing_percentage = (merged_df.isnull().sum() / len(merged_df)) * 100\n",
    "\n",
    "# Display the missing values percentage for each column\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker_x              0\n",
      "Company Name          0\n",
      "IndustryId            0\n",
      "Number Employees      0\n",
      "Date                  0\n",
      "Open                  0\n",
      "High                  0\n",
      "Low                   0\n",
      "Close                 0\n",
      "Adj. Close            0\n",
      "Volume                0\n",
      "Dividend              0\n",
      "Shares Outstanding    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing IndustryId with -1 (as a placeholder)\n",
    "merged_df[\"IndustryId\"].fillna(-1, inplace=True)\n",
    "\n",
    "# Fill Number Employees with the mean per IndustryId\n",
    "merged_df[\"Number Employees\"] = merged_df.groupby(\"IndustryId\")[\"Number Employees\"].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Fill Shares Outstanding with the mean per IndustryId\n",
    "merged_df[\"Shares Outstanding\"] = merged_df.groupby(\"IndustryId\")[\"Shares Outstanding\"].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Check if any missing values remain\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Unique Tickers: ['AAPL' 'AMZN' 'GOOG' 'META' 'MSFT' 'NVDA' 'TSLA']\n",
      "      Ticker_x Company Name  IndustryId  Number Employees       Date   Open  \\\n",
      "14268     AAPL    APPLE INC    101001.0          147000.0 2019-04-05  49.11   \n",
      "14269     AAPL    APPLE INC    101001.0          147000.0 2019-04-08  49.10   \n",
      "14270     AAPL    APPLE INC    101001.0          147000.0 2019-04-09  50.08   \n",
      "14271     AAPL    APPLE INC    101001.0          147000.0 2019-04-10  49.67   \n",
      "14272     AAPL    APPLE INC    101001.0          147000.0 2019-04-11  50.21   \n",
      "\n",
      "        High    Low  Close  Adj. Close     Volume  Dividend  \\\n",
      "14268  49.27  48.98  49.25       47.19   74106576       0.0   \n",
      "14269  50.06  49.09  50.02       47.93  103526788       0.0   \n",
      "14270  50.71  49.81  49.88       47.79  143072948       0.0   \n",
      "14271  50.19  49.55  50.16       48.06   86781152       0.0   \n",
      "14272  50.25  49.61  49.74       47.66   83603232       0.0   \n",
      "\n",
      "       Shares Outstanding  \n",
      "14268        1.842914e+10  \n",
      "14269        1.842914e+10  \n",
      "14270        1.842914e+10  \n",
      "14271        1.842914e+10  \n",
      "14272        1.842914e+10  \n"
     ]
    }
   ],
   "source": [
    "magnificent_7_tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"GOOG\", \"AMZN\", \"NVDA\", \"TSLA\", \"META\"]\n",
    "# Filter dataset to only include the Magnificent 7 tickers\n",
    "filtered_df = merged_df[merged_df[\"Ticker_x\"].isin(magnificent_7_tickers)]\n",
    "\n",
    "# Display the unique tickers in the filtered dataset (as a final check)\n",
    "print(\"Filtered Unique Tickers:\", filtered_df[\"Ticker_x\"].unique())\n",
    "\n",
    "# Display first few rows to confirm filtering\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to: data/magnificent_7_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "# Define output file path (change the path as needed)\n",
    "output_path = \"data/magnificent_7_filtered.csv\"  # Adjust this path to your preferred location\n",
    "\n",
    "# Save the filtered dataset to a CSV file\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Confirm save location\n",
    "print(f\"Filtered dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies in dataset: dict_keys(['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA', 'TSLA'])\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of DataFrames, one for each company\n",
    "company_dfs = {ticker: filtered_df[filtered_df[\"Ticker_x\"] == ticker] for ticker in filtered_df[\"Ticker_x\"].unique()}\n",
    "\n",
    "# Display the tickers to verify\n",
    "print(\"Companies in dataset:\", company_dfs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select Apple's DataFrame\n",
    "apple_df = company_dfs[\"AAPL\"]\n",
    "\n",
    "# Drop non-numeric columns before correlation analysis\n",
    "numeric_apple_df = apple_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = numeric_apple_df.corr()\n",
    "\n",
    "# Display correlation matrix visually\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Apple Stock Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print correlation values sorted by Close\n",
    "correlation_with_close = correlation_matrix[\"Close\"].sort_values(ascending=False)\n",
    "print(correlation_with_close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Ticker_x Company Name       Date  Close     Volume  Shares Outstanding\n",
      "14268     AAPL    APPLE INC 2019-04-05  49.25   74106576        1.842914e+10\n",
      "14269     AAPL    APPLE INC 2019-04-08  50.02  103526788        1.842914e+10\n",
      "14270     AAPL    APPLE INC 2019-04-09  49.88  143072948        1.842914e+10\n",
      "14271     AAPL    APPLE INC 2019-04-10  50.16   86781152        1.842914e+10\n",
      "14272     AAPL    APPLE INC 2019-04-11  49.74   83603232        1.842914e+10\n",
      "Remaining columns: Index(['Ticker_x', 'Company Name', 'Date', 'Close', 'Volume',\n",
      "       'Shares Outstanding'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = [\n",
    "    \"IndustryId\", \n",
    "    \"Number Employees\", \n",
    "    \"Open\", \n",
    "    \"High\", \n",
    "    \"Low\", \n",
    "    \"Adj. Close\", \n",
    "    \"Dividend\"\n",
    "]\n",
    "\n",
    "# Drop the columns from the general filtered_df\n",
    "filtered_df = filtered_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the updated dataset to verify\n",
    "print(filtered_df.head())\n",
    "print(\"Remaining columns:\", filtered_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies in dataset: dict_keys(['AAPL', 'AMZN', 'GOOG', 'META', 'MSFT', 'NVDA', 'TSLA'])\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of DataFrames, one for each company\n",
    "company_dfs = {ticker: filtered_df[filtered_df[\"Ticker_x\"] == ticker] for ticker in filtered_df[\"Ticker_x\"].unique()}\n",
    "\n",
    "# Display the tickers to verify\n",
    "print(\"Companies in dataset:\", company_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m apple_df \u001b[38;5;241m=\u001b[39m company_dfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Drop non-numeric columns before correlation analysis\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m numeric_apple_df \u001b[38;5;241m=\u001b[39m apple_df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mnumber])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute correlation matrix\u001b[39;00m\n\u001b[1;32m      8\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m numeric_apple_df\u001b[38;5;241m.\u001b[39mcorr()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Select Apple's DataFrame\n",
    "apple_df = company_dfs[\"AAPL\"]\n",
    "\n",
    "# Drop non-numeric columns before correlation analysis\n",
    "numeric_apple_df = apple_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = numeric_apple_df.corr()\n",
    "\n",
    "# Display correlation matrix visually\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Apple Stock Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print correlation values sorted by Close\n",
    "correlation_with_close = correlation_matrix[\"Close\"].sort_values(ascending=False)\n",
    "print(correlation_with_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the target variable (binary classification: 1 = Price Up, 0 = Price Down)\n",
    "apple_df = apple_df.sort_values(by=\"Date\")  # Ensure correct time order\n",
    "apple_df[\"Price_Up\"] = (apple_df[\"Close\"].shift(-1) > apple_df[\"Close\"]).astype(int)\n",
    "# Calculate daily price changes\n",
    "apple_df[\"Price_Change\"] = apple_df[\"Close\"].pct_change() * 100\n",
    "\n",
    "# Drop last row (no next-day price available)\n",
    "apple_df = apple_df.dropna()\n",
    "\n",
    "# Select features (Close price, Volume, Shares Outstanding)\n",
    "features = [\"Close\", \"Volume\", \"Shares Outstanding\"]\n",
    "X = apple_df[features]\n",
    "y = apple_df[\"Price_Up\"]\n",
    "\n",
    "# Split into training (80%) and testing (20%) - Keep chronological order\n",
    "train_size = int(len(apple_df) * 0.8)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# Standardize the features (important for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Accuracy Scores: [0.52763819 0.55276382 0.53535354 0.5        0.54040404]\n",
      "Mean CV Accuracy: 0.5312\n",
      "Test Set Accuracy: 0.5040\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.33      0.39       122\n",
      "           1       0.51      0.67      0.58       126\n",
      "\n",
      "    accuracy                           0.50       248\n",
      "   macro avg       0.50      0.50      0.49       248\n",
      "weighted avg       0.50      0.50      0.49       248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Train the model on the full training set\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "print(f\"Cross-validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully to log_reg_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"pkl/apple_log_reg_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(log_reg, file)\n",
    "\n",
    "print(\"Model saved successfully to log_reg_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try doing gradient descent to increase accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Company - AMZN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Apple's DataFrame\n",
    "amazon_df = company_dfs[\"AMZN\"]\n",
    "\n",
    "# Drop non-numeric columns before correlation analysis\n",
    "numeric_amazon_df = amazon_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = numeric_amazon_df.corr()\n",
    "\n",
    "# Display correlation matrix visually\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Amazon Stock Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print correlation values sorted by Close\n",
    "correlation_with_close = correlation_matrix[\"Close\"].sort_values(ascending=False)\n",
    "print(correlation_with_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the target variable (binary classification: 1 = Price Up, 0 = Price Down)\n",
    "amazon_df = amazon_df.sort_values(by=\"Date\")  # Ensure correct time order\n",
    "amazon_df[\"Price_Up\"] = (amazon_df[\"Close\"].shift(-1) > amazon_df[\"Close\"]).astype(int)\n",
    "# Calculate daily price changes\n",
    "amazon_df[\"Price_Change\"] = amazon_df[\"Close\"].pct_change() * 100\n",
    "\n",
    "# Drop last row (no next-day price available)\n",
    "amazon_df = amazon_df.dropna()\n",
    "\n",
    "# Select features (Close price, Volume, Shares Outstanding)\n",
    "features = [\"Close\", \"Volume\", \"Shares Outstanding\"]\n",
    "X = amazon_df[features]\n",
    "y = amazon_df[\"Price_Up\"]\n",
    "\n",
    "# Split into training (80%) and testing (20%) - Keep chronological order\n",
    "train_size = int(len(amazon_df) * 0.8)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# Standardize the features (important for Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Train the model on the full training set\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "print(f\"Cross-validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open(\"pkl/amazon_log_reg_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(log_reg, file)\n",
    "\n",
    "print(\"Model saved successfully to log_reg_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results aren't very good, so we move on to a different kind of model. \n",
    "# Time Series!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# from statsmodels.tsa.stattools import adfuller\\n\\n# Run ADF test on Close prices\\n# adf_result = adfuller(apple_df[\"Close\"])\\n\\n# Print ADF results\\n# print(\"ADF Statistic:\", adf_result[0])\\n# print(\"p-value:\", adf_result[1])\\n# if adf_result[1] > 0.05:\\n#    print(\"Data is non-stationary. Differencing is required.\")\\n# else:\\n#    print(\"Data is stationary.\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run ADF test on Close prices\n",
    "# adf_result = adfuller(apple_df[\"Close\"])\n",
    "\n",
    "# Print ADF results\n",
    "# print(\"ADF Statistic:\", adf_result[0])\n",
    "# print(\"p-value:\", adf_result[1])\n",
    "# if adf_result[1] > 0.05:\n",
    "#    print(\"Data is non-stationary. Differencing is required.\")\n",
    "# else:\n",
    "#    print(\"Data is stationary.\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apply first difference if necessary\\napple_df[\"Close_diff\"] = apple_df[\"Close\"].diff()\\n\\n# Drop NaN values (since differencing creates NaNs in the first row)\\napple_df = apple_df.dropna()\\n\\n# Re-run ADF test on differenced data\\nadf_result_diff = adfuller(apple_df[\"Close_diff\"])\\n\\n# Print results\\nprint(\"ADF Statistic (Differenced):\", adf_result_diff[0])\\nprint(\"p-value:\", adf_result_diff[1])\\nif adf_result_diff[1] > 0.05:\\n    print(\"Differenced data is still non-stationary. Further differencing may be required.\")\\nelse:\\n    print(\"Differenced data is now stationary.\")\\n    '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Apply first difference if necessary\n",
    "apple_df[\"Close_diff\"] = apple_df[\"Close\"].diff()\n",
    "\n",
    "# Drop NaN values (since differencing creates NaNs in the first row)\n",
    "apple_df = apple_df.dropna()\n",
    "\n",
    "# Re-run ADF test on differenced data\n",
    "adf_result_diff = adfuller(apple_df[\"Close_diff\"])\n",
    "\n",
    "# Print results\n",
    "print(\"ADF Statistic (Differenced):\", adf_result_diff[0])\n",
    "print(\"p-value:\", adf_result_diff[1])\n",
    "if adf_result_diff[1] > 0.05:\n",
    "    print(\"Differenced data is still non-stationary. Further differencing may be required.\")\n",
    "else:\n",
    "    print(\"Differenced data is now stationary.\")\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\\n\\n# ACF (Autocorrelation Function) - Checks how past values influence future values\\nplot_acf(apple_df[\"Close_diff\"], lags=20)\\nplt.show()\\n\\n# PACF (Partial Autocorrelation Function) - Identifies the direct impact of past values\\nplot_pacf(apple_df[\"Close_diff\"], lags=20)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# ACF (Autocorrelation Function) - Checks how past values influence future values\n",
    "plot_acf(apple_df[\"Close_diff\"], lags=20)\n",
    "plt.show()\n",
    "\n",
    "# PACF (Partial Autocorrelation Function) - Identifies the direct impact of past values\n",
    "plot_pacf(apple_df[\"Close_diff\"], lags=20)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we get to the point where we have stationary data, congratulations. \n",
    "# But it also shows that the previous day's price has little correlation with the next day's price. So we must add additional variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport talib\\n# Define target variable: Will the differenced price be positive?\\napple_df[\"Price_Up\"] = (apple_df[\"Close_diff\"] > 0).astype(int)\\n\\n# Compute technical indicators\\napple_df[\"SMA_5\"] = apple_df[\"Close_diff\"].rolling(window=5).mean()\\napple_df[\"SMA_10\"] = apple_df[\"Close_diff\"].rolling(window=10).mean()\\napple_df[\"Volatility\"] = apple_df[\"Close_diff\"].rolling(window=5).std()\\napple_df[\"RSI\"] = talib.RSI(apple_df[\"Close\"], timeperiod=14)\\napple_df[\"MACD\"], _, _ = talib.MACD(apple_df[\"Close\"])\\n\\n# Drop NaN values caused by rolling calculations\\napple_df = apple_df.dropna()\\n\\n# Compute correlation matrix\\ncorrelation_matrix = apple_df[[\"Price_Up\", \"Close_diff\", \"SMA_5\", \"SMA_10\", \"Volatility\", \"RSI\", \"MACD\"]].corr()\\n\\n# Display correlation values sorted by Price_Up\\ncorrelation_with_target = correlation_matrix[\"Price_Up\"].sort_values(ascending=False)\\nprint(\"Correlation with Price_Up:\\n\", correlation_with_target)\\n\\n# Visualize correlation matrix\\nplt.figure(figsize=(10, 6))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\\nplt.title(\"Correlation Matrix - Apple Stock (Fixed)\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import talib\n",
    "# Define target variable: Will the differenced price be positive?\n",
    "apple_df[\"Price_Up\"] = (apple_df[\"Close_diff\"] > 0).astype(int)\n",
    "\n",
    "# Compute technical indicators\n",
    "apple_df[\"SMA_5\"] = apple_df[\"Close_diff\"].rolling(window=5).mean()\n",
    "apple_df[\"SMA_10\"] = apple_df[\"Close_diff\"].rolling(window=10).mean()\n",
    "apple_df[\"Volatility\"] = apple_df[\"Close_diff\"].rolling(window=5).std()\n",
    "apple_df[\"RSI\"] = talib.RSI(apple_df[\"Close\"], timeperiod=14)\n",
    "apple_df[\"MACD\"], _, _ = talib.MACD(apple_df[\"Close\"])\n",
    "\n",
    "# Drop NaN values caused by rolling calculations\n",
    "apple_df = apple_df.dropna()\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = apple_df[[\"Price_Up\", \"Close_diff\", \"SMA_5\", \"SMA_10\", \"Volatility\", \"RSI\", \"MACD\"]].corr()\n",
    "\n",
    "# Display correlation values sorted by Price_Up\n",
    "correlation_with_target = correlation_matrix[\"Price_Up\"].sort_values(ascending=False)\n",
    "print(\"Correlation with Price_Up:\\n\", correlation_with_target)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix - Apple Stock (Fixed)\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Drop weakly correlated features\\nfeatures_to_drop = [ \"Volatility\", \"MACD\"]\\napple_df = apple_df.drop(columns=features_to_drop)\\n\\n# Verify remaining features\\nprint(\"Remaining features:\", apple_df.columns)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Drop weakly correlated features\n",
    "features_to_drop = [ \"Volatility\", \"MACD\"]\n",
    "apple_df = apple_df.drop(columns=features_to_drop)\n",
    "\n",
    "# Verify remaining features\n",
    "print(\"Remaining features:\", apple_df.columns)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score, classification_report\\n\\n# Define features and target variable\\nfeatures = [\"SMA_5\", \"SMA_10\", \"RSI\"]\\nX = apple_df[features]\\ny = apple_df[\"Price_Up\"]\\n\\n# Split into training (80%) and testing (20%) - Keep chronological order\\ntrain_size = int(len(apple_df) * 0.8)\\nX_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\\ny_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\\n\\n# Standardize the features\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Define Logistic Regression model\\nlog_reg = LogisticRegression()\\n\\n# Set up 5-fold cross-validation\\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\\n# Perform cross-validation\\ncv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=kf, scoring=\"accuracy\")\\n\\n# Train the model on full training set\\nlog_reg.fit(X_train_scaled, y_train)\\n\\n# Predictions on the test set\\ny_pred = log_reg.predict(X_test_scaled)\\n\\n# Evaluate Model Performance\\nprint(f\"Cross-validation Accuracy Scores: {cv_scores}\")\\nprint(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\\nprint(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define features and target variable\n",
    "features = [\"SMA_5\", \"SMA_10\", \"RSI\"]\n",
    "X = apple_df[features]\n",
    "y = apple_df[\"Price_Up\"]\n",
    "\n",
    "# Split into training (80%) and testing (20%) - Keep chronological order\n",
    "train_size = int(len(apple_df) * 0.8)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Train the model on full training set\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "print(f\"Cross-validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Ensure DataFrame has a proper integer index\\napple_df = apple_df.reset_index(drop=True)\\n\\n# Initialize Signal column as Hold\\napple_df[\"Signal\"] = \"Hold\"  \\n\\nholding = False  # Tracks whether we currently own the stock\\n\\n# Iterate through each row except the last\\nfor i in range(len(apple_df) - 1):  \\n    if pd.notna(apple_df.loc[i, \"Close\"]):  # Use .loc[] instead of .at[]\\n        if apple_df.loc[i, \"Price_Up\"] == 1:  # Price predicted to go up\\n            if not holding:  \\n                apple_df.loc[i, \"Signal\"] = \"Buy\"\\n                holding = True  # We now own the stock\\n            else:\\n                apple_df.loc[i, \"Signal\"] = \"Hold\"\\n        else:  # Price predicted to go down\\n            if holding:\\n                apple_df.loc[i, \"Signal\"] = \"Sell\"\\n                holding = False  # We no longer own the stock\\n            else:\\n                apple_df.loc[i, \"Signal\"] = \"Hold\"\\n\\n# Display sample of updated Signal column\\nprint(apple_df[[\"Date\", \"Close\", \"Price_Up\", \"Signal\"]].tail(20))\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Ensure DataFrame has a proper integer index\n",
    "apple_df = apple_df.reset_index(drop=True)\n",
    "\n",
    "# Initialize Signal column as Hold\n",
    "apple_df[\"Signal\"] = \"Hold\"  \n",
    "\n",
    "holding = False  # Tracks whether we currently own the stock\n",
    "\n",
    "# Iterate through each row except the last\n",
    "for i in range(len(apple_df) - 1):  \n",
    "    if pd.notna(apple_df.loc[i, \"Close\"]):  # Use .loc[] instead of .at[]\n",
    "        if apple_df.loc[i, \"Price_Up\"] == 1:  # Price predicted to go up\n",
    "            if not holding:  \n",
    "                apple_df.loc[i, \"Signal\"] = \"Buy\"\n",
    "                holding = True  # We now own the stock\n",
    "            else:\n",
    "                apple_df.loc[i, \"Signal\"] = \"Hold\"\n",
    "        else:  # Price predicted to go down\n",
    "            if holding:\n",
    "                apple_df.loc[i, \"Signal\"] = \"Sell\"\n",
    "                holding = False  # We no longer own the stock\n",
    "            else:\n",
    "                apple_df.loc[i, \"Signal\"] = \"Hold\"\n",
    "\n",
    "# Display sample of updated Signal column\n",
    "print(apple_df[[\"Date\", \"Close\", \"Price_Up\", \"Signal\"]].tail(20))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the file path (update the path as needed)\\noutput_path = \"data/apple_stock_predictions.csv\"\\n\\n# Save the DataFrame as a CSV file\\napple_df.to_csv(output_path, index=False)\\n\\n# Confirm save location\\nprint(f\"Apple stock predictions saved to: {output_path}\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the file path (update the path as needed)\n",
    "output_path = \"data/apple_stock_predictions.csv\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "apple_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Confirm save location\n",
    "print(f\"Apple stock predictions saved to: {output_path}\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalmaufc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
