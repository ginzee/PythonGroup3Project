{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing Mag 7 data from ZIP...\n",
      "Feature stats:\n",
      "             close    p_e_ratio       sma_50\n",
      "count  8127.000000  8127.000000  8127.000000\n",
      "mean    155.364700   193.980066   152.022184\n",
      "std      97.308702   138.756884    94.996129\n",
      "min       3.620000    27.990842     3.870600\n",
      "25%      86.950000    98.693750    78.956600\n",
      "50%     146.140000   129.085524   143.678800\n",
      "75%     218.470000   246.427372   216.149400\n",
      "max     502.300000   500.000000   411.813400\n",
      "Saved mag7_processed_final3.csv locally.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "MAGNIFICENT_7 = {'AAPL', 'MSFT', 'GOOG', 'AMZN', 'NVDA', 'META', 'TSLA'}\n",
    "\n",
    "CACHE = {}\n",
    "\n",
    "def extract_and_load(zip_path: str, filenames: list):\n",
    "    \"\"\"Extracts files from ZIP and loads them into DataFrames. Uses caching.\"\"\"\n",
    "    global CACHE\n",
    "    \n",
    "    if zip_path in CACHE:\n",
    "        print(\"Using cached data...\")\n",
    "        return CACHE[zip_path]\n",
    "    \n",
    "    dataframes = {}\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for filename in filenames:\n",
    "            with zip_ref.open(filename) as file:\n",
    "                dataframes[filename] = pd.read_csv(file, delimiter=';', header=0)\n",
    "    \n",
    "    CACHE[zip_path] = dataframes\n",
    "    return dataframes\n",
    "\n",
    "def load_and_process(zip_path):\n",
    "    print(\"Loading and processing Mag 7 data from ZIP...\")\n",
    "    \n",
    "    filenames = ['us-shareprices-daily.csv', 'us-income-quarterly.csv', 'us-balance-quarterly.csv']\n",
    "    data = extract_and_load(zip_path, filenames)\n",
    "    \n",
    "    df = data['us-shareprices-daily.csv']\n",
    "    df['date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Ticker'].isin(MAGNIFICENT_7)][['Ticker', 'date', 'Close']].rename(columns={'Close': 'close'})\n",
    "    \n",
    "    income_df = data['us-income-quarterly.csv']\n",
    "    income_df['date'] = pd.to_datetime(income_df['Report Date'])\n",
    "    income_df = income_df[income_df['Ticker'].isin(MAGNIFICENT_7)][['Ticker', 'date', 'Net Income', 'Shares (Basic)']]\n",
    "    income_df['p_e_ratio'] = np.nan\n",
    "    \n",
    "    balance_df = data['us-balance-quarterly.csv']\n",
    "    balance_df['date'] = pd.to_datetime(balance_df['Report Date'])\n",
    "    balance_df = balance_df[balance_df['Ticker'].isin(MAGNIFICENT_7)][['Ticker', 'date', 'Total Liabilities', 'Total Equity']]\n",
    "    \n",
    "    merged_df = df.merge(income_df, on=['Ticker', 'date'], how='left')\\\n",
    "                  .merge(balance_df, on=['Ticker', 'date'], how='left')\\\n",
    "                  .sort_values(['Ticker', 'date'])\\\n",
    "                  .ffill()\n",
    "\n",
    "    merged_df['p_e_ratio'] = merged_df['close'] / (merged_df['Net Income'] / merged_df['Shares (Basic)'].replace(0, np.nan))\n",
    "    merged_df['p_e_ratio'] = merged_df['p_e_ratio'].where(merged_df['p_e_ratio'] >= 0, np.nan).clip(upper=500)\n",
    "\n",
    "    merged_df['sma_50'] = merged_df.groupby('Ticker')['close'].rolling(window=50, min_periods=50).mean().reset_index(drop=True)\n",
    "    merged_df['next_day_close'] = merged_df.groupby('Ticker')['close'].shift(-1)\n",
    "    merged_df['next_day_direction'] = (merged_df['next_day_close'] > merged_df['close']).astype(int)\n",
    "\n",
    "    feature_cols = ['close', 'p_e_ratio', 'sma_50']\n",
    "    for col in feature_cols:\n",
    "        merged_df[col] = merged_df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    merged_df = merged_df.dropna(subset=feature_cols + ['next_day_direction'])\n",
    "\n",
    "    print(\"Feature stats:\")\n",
    "    print(merged_df[feature_cols].describe())\n",
    "\n",
    "    # Save the processed dataset locally\n",
    "    merged_df.to_csv('mag7_processed_final3.csv', index=False)\n",
    "    print(\"Saved mag7_processed_final3.csv locally.\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def main():\n",
    "    zip_path = 'data/mag7_data.zip'  # Define zip_path here so it is accessible globally\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Error: ZIP file not found at {zip_path}\")\n",
    "        return\n",
    "    \n",
    "    result = load_and_process(zip_path)  # Now zip_path is correctly passed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalmaufc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
